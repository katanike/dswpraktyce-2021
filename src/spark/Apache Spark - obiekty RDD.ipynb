{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podstawy Apache Spark - obiekty RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# jeśli w systemie jest ustawiona zmienna SPARK_HOME to można wykonać:\n",
    "findspark.init()\n",
    "# jeśli nie jest to w ten sposób\n",
    "# findspark.init(\"katalog-ze-sparkiem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"HelloSpark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-0HM28J7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>HelloSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=HelloSpark>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [1,2,3,4,5,6]\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[13] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(arr)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x: x%2==0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_power = rdd.map(lambda x: x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_power.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(x):\n",
    "    return x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_power = rdd.map(power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_power2 = rdd_power.map(power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 16, 81, 256, 625, 1296]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_power2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4294967296,\n",
       " 1853020188851841,\n",
       " 18446744073709551616,\n",
       " 23283064365386962890625,\n",
       " 7958661109946400884391936]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_power.map(power).map(power).map(power).map(power).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rdd = sc.textFile(\"c:/opt/spark-3.1.1-bin-hadoop2.7/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Apache Spark'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_rdd = file_rdd.filter(lambda linia: len(linia) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liczba_niepustych_lini = filtered_rdd.count()\n",
    "liczba_niepustych_lini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " '## Interactive Python Shell',\n",
       " 'Alternatively, if you prefer Python, you can use the Python shell:']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_rdd.filter(lambda linia: \"Python\" in linia).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = filtered_rdd.map(lambda linia: linia.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 12, 13, 11, 12, 8, 6, 1, 2, 2]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda slowa: len(slowa)).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_flat_rdd = filtered_rdd.flatMap(lambda linia: linia.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'a',\n",
       " 'unified',\n",
       " 'analytics',\n",
       " 'engine',\n",
       " 'for']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_flat_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_flat_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_flat_rdd.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"for\", \"is\", \"the\", \"of\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_words = words_flat_rdd.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_words = distinct_words.filter(lambda w: len(w) > 1).filter(lambda w: w not in stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ok_words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache', 'Spark', 'unified', 'analytics', 'engine']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ok_words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_words.saveAsTextFile(\"c:/opt/spark-3.1.1-bin-hadoop2.7/words_ok.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tuples = words_flat_rdd.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1), ('Apache', 1), ('Spark', 1), ('Spark', 1), ('is', 1)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tuples.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1),\n",
       " ('Apache', 1),\n",
       " ('Spark', 14),\n",
       " ('is', 7),\n",
       " ('unified', 1),\n",
       " ('analytics', 1),\n",
       " ('engine', 2),\n",
       " ('It', 2),\n",
       " ('provides', 1),\n",
       " ('high-level', 1),\n",
       " ('APIs', 1),\n",
       " ('in', 5),\n",
       " ('Scala,', 1),\n",
       " ('Java,', 1),\n",
       " ('an', 4),\n",
       " ('optimized', 1),\n",
       " ('supports', 2),\n",
       " ('computation', 1),\n",
       " ('analysis.', 1),\n",
       " ('set', 2),\n",
       " ('of', 5),\n",
       " ('tools', 1),\n",
       " ('SQL', 2),\n",
       " ('MLlib', 1),\n",
       " ('machine', 1),\n",
       " ('learning,', 1),\n",
       " ('GraphX', 1),\n",
       " ('graph', 1),\n",
       " ('processing,', 1),\n",
       " ('Structured', 1),\n",
       " ('<https://spark.apache.org/>', 1),\n",
       " ('Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
       "  1),\n",
       " ('Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)',\n",
       "  1),\n",
       " ('Documentation', 1),\n",
       " ('latest', 1),\n",
       " ('programming', 1),\n",
       " ('guide,', 1),\n",
       " ('[project', 1),\n",
       " ('page](https://spark.apache.org/documentation.html).', 1),\n",
       " ('README', 1),\n",
       " ('only', 1),\n",
       " ('basic', 1),\n",
       " ('instructions.', 1),\n",
       " ('Building', 1),\n",
       " ('using', 3),\n",
       " ('[Apache', 1),\n",
       " ('run:', 1),\n",
       " ('do', 2),\n",
       " ('this', 1),\n",
       " ('downloaded', 1),\n",
       " ('documentation', 3),\n",
       " ('project', 1),\n",
       " ('site,', 1),\n",
       " ('at', 2),\n",
       " ('Spark\"](https://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
       " ('development', 1),\n",
       " ('tips,', 1),\n",
       " ('developing', 1),\n",
       " ('IDE,', 1),\n",
       " ('[\"Useful', 1),\n",
       " ('Developer', 1),\n",
       " ('Interactive', 2),\n",
       " ('Shell', 2),\n",
       " ('The', 1),\n",
       " ('way', 1),\n",
       " ('start', 1),\n",
       " ('Try', 1),\n",
       " ('following', 2),\n",
       " ('scala>', 1),\n",
       " ('spark.range(1000', 2),\n",
       " ('*', 4),\n",
       " ('1000).count()', 2),\n",
       " ('Python', 2),\n",
       " ('Alternatively,', 1),\n",
       " ('use', 3),\n",
       " ('And', 1),\n",
       " ('run', 7),\n",
       " ('Example', 1),\n",
       " ('several', 1),\n",
       " ('programs', 2),\n",
       " ('them,', 1),\n",
       " ('`./bin/run-example', 1),\n",
       " ('[params]`.', 1),\n",
       " ('example:', 1),\n",
       " ('./bin/run-example', 2),\n",
       " ('SparkPi', 2),\n",
       " ('variable', 1),\n",
       " ('when', 1),\n",
       " ('examples', 2),\n",
       " ('spark://', 1),\n",
       " ('URL,', 1),\n",
       " ('YARN,', 1),\n",
       " ('\"local\"', 1),\n",
       " ('locally', 2),\n",
       " ('N', 1),\n",
       " ('abbreviated', 1),\n",
       " ('class', 2),\n",
       " ('name', 1),\n",
       " ('package.', 1),\n",
       " ('instance:', 1),\n",
       " ('print', 1),\n",
       " ('usage', 1),\n",
       " ('help', 1),\n",
       " ('no', 1),\n",
       " ('params', 1),\n",
       " ('are', 1),\n",
       " ('Testing', 1),\n",
       " ('Spark](#building-spark).', 1),\n",
       " ('Once', 1),\n",
       " ('built,', 1),\n",
       " ('tests', 2),\n",
       " ('using:', 1),\n",
       " ('./dev/run-tests', 1),\n",
       " ('Please', 4),\n",
       " ('guidance', 2),\n",
       " ('module,', 1),\n",
       " ('individual', 1),\n",
       " ('integration', 1),\n",
       " ('test,', 1),\n",
       " ('Note', 1),\n",
       " ('About', 1),\n",
       " ('uses', 1),\n",
       " ('library', 1),\n",
       " ('HDFS', 1),\n",
       " ('other', 1),\n",
       " ('Hadoop-supported', 1),\n",
       " ('storage', 1),\n",
       " ('systems.', 1),\n",
       " ('Because', 1),\n",
       " ('have', 1),\n",
       " ('changed', 1),\n",
       " ('different', 1),\n",
       " ('versions', 1),\n",
       " ('Hadoop,', 2),\n",
       " ('must', 1),\n",
       " ('against', 1),\n",
       " ('version', 1),\n",
       " ('refer', 2),\n",
       " ('YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
       "  1),\n",
       " ('particular', 2),\n",
       " ('distribution', 1),\n",
       " ('Hive', 2),\n",
       " ('Thriftserver', 1),\n",
       " ('distributions.', 1),\n",
       " ('[Configuration', 1),\n",
       " ('online', 1),\n",
       " ('overview', 1),\n",
       " ('configure', 1),\n",
       " ('Spark.', 1),\n",
       " ('Contributing', 1),\n",
       " ('guide](https://spark.apache.org/contributing.html)', 1),\n",
       " ('started', 1),\n",
       " ('contributing', 1),\n",
       " ('project.', 1),\n",
       " ('a', 9),\n",
       " ('for', 12),\n",
       " ('large-scale', 1),\n",
       " ('data', 2),\n",
       " ('processing.', 2),\n",
       " ('Python,', 2),\n",
       " ('and', 9),\n",
       " ('R,', 1),\n",
       " ('that', 2),\n",
       " ('general', 2),\n",
       " ('graphs', 1),\n",
       " ('also', 5),\n",
       " ('rich', 1),\n",
       " ('higher-level', 1),\n",
       " ('including', 4),\n",
       " ('DataFrames,', 1),\n",
       " ('Streaming', 1),\n",
       " ('stream', 1),\n",
       " ('[![Jenkins', 1),\n",
       " ('Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3)',\n",
       "  1),\n",
       " ('[![AppVeyor', 1),\n",
       " ('[![PySpark', 1),\n",
       " ('##', 9),\n",
       " ('Online', 1),\n",
       " ('You', 3),\n",
       " ('can', 6),\n",
       " ('find', 1),\n",
       " ('the', 23),\n",
       " ('documentation,', 1),\n",
       " ('on', 7),\n",
       " ('web', 1),\n",
       " ('This', 2),\n",
       " ('file', 1),\n",
       " ('contains', 1),\n",
       " ('setup', 1),\n",
       " ('built', 1),\n",
       " ('Maven](https://maven.apache.org/).', 1),\n",
       " ('To', 2),\n",
       " ('build', 3),\n",
       " ('its', 1),\n",
       " ('example', 3),\n",
       " ('programs,', 1),\n",
       " ('./build/mvn', 1),\n",
       " ('-DskipTests', 1),\n",
       " ('clean', 1),\n",
       " ('package', 1),\n",
       " ('(You', 1),\n",
       " ('not', 1),\n",
       " ('need', 1),\n",
       " ('to', 16),\n",
       " ('if', 4),\n",
       " ('you', 4),\n",
       " ('pre-built', 1),\n",
       " ('package.)', 1),\n",
       " ('More', 1),\n",
       " ('detailed', 2),\n",
       " ('available', 1),\n",
       " ('from', 1),\n",
       " ('[\"Building', 1),\n",
       " ('For', 3),\n",
       " ('info', 1),\n",
       " ('see', 3),\n",
       " ('Tools\"](https://spark.apache.org/developer-tools.html).', 1),\n",
       " ('Scala', 2),\n",
       " ('easiest', 1),\n",
       " ('through', 1),\n",
       " ('shell:', 2),\n",
       " ('./bin/spark-shell', 1),\n",
       " ('command,', 2),\n",
       " ('which', 2),\n",
       " ('should', 2),\n",
       " ('return', 2),\n",
       " ('1,000,000,000:', 2),\n",
       " ('1000', 2),\n",
       " ('prefer', 1),\n",
       " ('./bin/pyspark', 1),\n",
       " ('>>>', 1),\n",
       " ('Programs', 1),\n",
       " ('comes', 1),\n",
       " ('with', 3),\n",
       " ('sample', 1),\n",
       " ('`examples`', 2),\n",
       " ('directory.', 1),\n",
       " ('one', 2),\n",
       " ('<class>', 1),\n",
       " ('will', 1),\n",
       " ('Pi', 1),\n",
       " ('locally.', 1),\n",
       " ('MASTER', 1),\n",
       " ('environment', 1),\n",
       " ('running', 1),\n",
       " ('submit', 1),\n",
       " ('cluster.', 1),\n",
       " ('be', 2),\n",
       " ('mesos://', 1),\n",
       " ('or', 3),\n",
       " ('\"yarn\"', 1),\n",
       " ('thread,', 1),\n",
       " ('\"local[N]\"', 1),\n",
       " ('threads.', 1),\n",
       " ('MASTER=spark://host:7077', 1),\n",
       " ('Many', 1),\n",
       " ('given.', 1),\n",
       " ('Running', 1),\n",
       " ('Tests', 1),\n",
       " ('first', 1),\n",
       " ('requires', 1),\n",
       " ('[building', 1),\n",
       " ('how', 3),\n",
       " ('[run', 1),\n",
       " ('tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n",
       "  1),\n",
       " ('There', 1),\n",
       " ('Kubernetes', 1),\n",
       " ('resource-managers/kubernetes/integration-tests/README.md', 1),\n",
       " ('A', 1),\n",
       " ('Hadoop', 3),\n",
       " ('Versions', 1),\n",
       " ('core', 1),\n",
       " ('talk', 1),\n",
       " ('protocols', 1),\n",
       " ('same', 1),\n",
       " ('your', 1),\n",
       " ('cluster', 1),\n",
       " ('runs.', 1),\n",
       " ('[\"Specifying', 1),\n",
       " ('Version', 1),\n",
       " ('Enabling', 1),\n",
       " ('building', 2),\n",
       " ('Configuration', 1),\n",
       " ('Guide](https://spark.apache.org/docs/latest/configuration.html)', 1),\n",
       " ('review', 1),\n",
       " ('[Contribution', 1),\n",
       " ('information', 1),\n",
       " ('get', 1)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tuples.reduceByKey(lambda a, b: a + b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
